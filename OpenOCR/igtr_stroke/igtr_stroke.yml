Global:
  device: gpu
  epoch_num: 100
  log_smooth_window: 20
  print_batch_step: 5000
  output_dir: ./output/rec/svtr_base_igtr_stroke_pt
  save_epoch_step: 5
  # evaluation is run every 2000 iterations
  eval_batch_step: [0, 5000]
  eval_epoch_step: [0, 5]
  cal_metric_during_train: True
  # pretrained_model: ./igtr_u14m_pt_stroke_model_log/best.pth
  # ./output/rec/igtr_u14m_pt_model_log/pretrain_model.pth
  checkpoints:
  use_tensorboard: false
  infer_img:
  # for data or label process
  # character_dict_path: &character_dict_path ./tools/utils/dict/vn_dict.txt
  character_dict_path: &character_dict_path ./tools/utils/dict/strokes/stroke_dict.txt
  # ./tools/utils/EN_symbol_dict.txt # 96en
  # ./tools/utils/ppocr_keys_v1.txt  # ch
  max_text_length: &max_text_length 25
  use_space_char: &use_space_char False
  save_res_path: ./output/rec/predicts_igtr_u14m_stroke_pt.txt
  use_amp: True

Optimizer:
  name: AdamW
  lr: 0.00015 # 2gpus 384bs/gpu
  weight_decay: 0.05
  filter_bias_and_bn: True

LRScheduler:
  name: OneCycleLR
  warmup_epoch: 1
  cycle_momentum: False

Architecture:
  model_type: rec
  algorithm: BGPD
  in_channels: 3
  Transform:
  Encoder:
    name: SVTRNet2DPos
    img_size: [32, -1]
    out_char_num: 25
    out_channels: 256
    patch_merging: 'Conv'
    embed_dim: [128, 256, 384]
    depth: [6, 6, 6]
    num_heads: [4, 8, 12]
    mixer: ['ConvB','ConvB','ConvB','ConvB','ConvB','ConvB', 'ConvB','ConvB', 'Global','Global','Global','Global','Global','Global','Global','Global','Global','Global']
    local_mixer: [[5, 5], [5, 5], [5, 5]]
    last_stage: False
    prenorm: True
    use_first_sub: False
  Decoder:
    name: IGTRDecoder
    dim: 384
    num_layer: 1
    ar: False
    refine_iter: 0
    # next_pred: True
    next_pred: False
    pos2d: True
    ds: True
    # pos_len: False
    # rec_layer: 1


Loss:
  name: IGTRLoss

PostProcess:
  name: IGTRLabelDecode
  character_dict_path: *character_dict_path
  use_space_char: *use_space_char

Metric:
  name: RecMetric
  main_indicator: acc
  is_filter: False

Train:
  dataset:
    name: RatioDataSet
    ds_width: True
    padding: &padding False
    data_dir_list: [
      /mlcv2/WorkingSpace/Personal/hamh/Ha/Data/Vicalligraphy/ViCalligraphy/lmdb_stroke_new/training
    ]
    transforms:
      - DecodeImage: # load image
          img_mode: BGR
          channel_first: False
      - PARSeqAug:
      - IGTRLabelEncode: # Class handling label
          k: 8
          prompt_error: False
          character_dict_path: *character_dict_path
          use_space_char: *use_space_char
          max_text_length: *max_text_length
      - KeepKeys:
          keep_keys: ['image', 'label', 'prompt_pos_idx_list',
          'prompt_char_idx_list', 'ques_pos_idx_list', 'ques1_answer_list',
          'ques2_char_idx_list', 'ques2_answer_list', 'ques3_answer', 'ques4_char_num_list',
          'ques_len_list', 'ques2_len_list', 'prompt_len_list', 'length'] # dataloader will return list in this order
  sampler:
    name: RatioSampler
    scales: [[128, 32]] # w, h
    # divide_factor: to ensure the width and height dimensions can be devided by downsampling multiple
    first_bs: &bs 32
    fix_bs: false
    divided_factor: [4, 16] # w, h
    is_training: True
  loader:
    shuffle: True
    batch_size_per_card: *bs
    drop_last: True
    max_ratio: &max_ratio 4
    num_workers: 4

Eval:
  dataset:
    name: RatioDataSet
    ds_width: True
    padding: *padding
    data_dir_list: [
      # /mlcv2/WorkingSpace/Personal/hamh/Ha/Data/Vicalligraphy/ViCalligraphy/evaluation/calligraphy 
      /mlcv2/WorkingSpace/Personal/hamh/Ha/Data/Vicalligraphy/ViCalligraphy/lmdb_stroke_new/evaluation
    ]
    transforms:
      - DecodeImage: # load image
          img_mode: BGR
          channel_first: False
      - ARLabelEncode: # Class handling label
          character_dict_path: *character_dict_path
          use_space_char: *use_space_char
          max_text_length: *max_text_length
      - KeepKeys:
          keep_keys: ['image', 'label', 'length'] # dataloader will return list in this order
  sampler:
    name: RatioSampler
    scales: [[128, 32]] # w, h
    # divide_factor: to ensure the width and height dimensions can be devided by downsampling multiple
    first_bs: 16
    fix_bs: false
    divided_factor: [4, 16] # w, h
    is_training: False
  loader:
    shuffle: False
    drop_last: False
    batch_size_per_card: 16
    max_ratio: *max_ratio
    num_workers: 4
