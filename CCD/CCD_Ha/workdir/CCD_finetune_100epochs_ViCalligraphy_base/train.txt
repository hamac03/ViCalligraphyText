ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 48
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 48
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 8
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 5000
	(73): training_show_iters = 100
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/training-->12431'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 259
Read vision model from pretrained_model/Base_ARD_checkpoint.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> train loss:5.4964752197265625
eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 32
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 32
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 8
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 5000
	(73): training_show_iters = 100
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/training-->12431'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 389
Read vision model from pretrained_model/Base_ARD_checkpoint.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> train loss:5.617279529571533
eval model
iteration:100--> train loss:3.670722007751465
iteration:200--> train loss:1.807703971862793
iteration:300--> train loss:1.2007230520248413
iteration:400--> train loss:1.0242178440093994
iteration:500--> train loss:0.9287381768226624
iteration:600--> train loss:0.8541532158851624
iteration:700--> train loss:0.8151190280914307
iteration:800--> train loss:0.7216145396232605
iteration:900--> train loss:0.6852792501449585
iteration:1000--> train loss:0.6633190512657166
iteration:1100--> train loss:0.6428961753845215
iteration:1200--> train loss:0.6296958327293396
iteration:1300--> train loss:0.582084059715271
iteration:1400--> train loss:0.5915635228157043
iteration:1500--> train loss:0.592312753200531
iteration:1600--> train loss:0.5451884865760803
iteration:1700--> train loss:0.5300147533416748
iteration:1800--> train loss:0.5560746192932129
iteration:1900--> train loss:0.5633668303489685
iteration:2000--> train loss:0.5161014795303345
iteration:2100--> train loss:0.5248324871063232
iteration:2200--> train loss:0.5124661326408386
iteration:2300--> train loss:0.5425049662590027
iteration:2400--> train loss:0.48830297589302063
iteration:2500--> train loss:0.4824105203151703
iteration:2600--> train loss:0.4785385727882385
iteration:2700--> train loss:0.49189722537994385
iteration:2800--> train loss:0.4686441421508789
iteration:2900--> train loss:0.4724653661251068
iteration:3000--> train loss:0.49504145979881287
iteration:3100--> train loss:0.48608943819999695
iteration:3200--> train loss:0.4662243127822876
iteration:3300--> train loss:0.4465436339378357
iteration:3400--> train loss:0.4591524302959442
iteration:3500--> train loss:0.47473353147506714
iteration:3600--> train loss:0.4158754348754883
iteration:3700--> train loss:0.44329988956451416
iteration:3800--> train loss:0.4684982895851135
iteration:3900--> train loss:0.47332990169525146
iteration:4000--> train loss:0.4013122022151947
iteration:4100--> train loss:0.421505868434906
iteration:4200--> train loss:0.4089890122413635
iteration:4300--> train loss:0.45604923367500305
iteration:4400--> train loss:0.399825781583786
iteration:4500--> train loss:0.4275222420692444
iteration:4600--> train loss:0.44357138872146606
iteration:4700--> train loss:0.4487624168395996
iteration:4800--> train loss:0.4339394271373749
iteration:4900--> train loss:0.42046794295310974
iteration:5000--> train loss:0.4191305935382843
eval model
iteration:5100--> train loss:0.39738041162490845
iteration:5200--> train loss:0.3702470064163208
iteration:5300--> train loss:0.3987272083759308
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 32
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 32
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 8
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 500
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 500
	(73): training_show_iters = 100
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from pretrained_model/Base_ARD_checkpoint.pth.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 32
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 32
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = saved_models/CCD_finetune_100epochs_ViCalligraphy_base/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_pretrain_checkpoint = ../../../data/CCD/Base_ARD_checkpoint.pth
	(53): model_strict = True
	(54): mp_num = 4
	(55): num_workers = 8
	(56): optimizer = adamw
	(57): optimizer_args_betas = (0.9, 0.999)
	(58): optimizer_bn_wd = False
	(59): optimizer_clip_grad = 20
	(60): optimizer_lr = 0.0001
	(61): optimizer_scheduler_gamma = 0.1
	(62): optimizer_scheduler_periods = [3, 1, 1]
	(63): optimizer_true_wd = False
	(64): optimizer_type = Adam
	(65): optimizer_wd = 0.0
	(66): out_dim = 65536
	(67): output_dir = ./saved_models/
	(68): patch_size = 4
	(69): seed = 0
	(70): training_epochs = 100
	(71): training_eval_iters = 500
	(72): training_hist_iters = 10000000
	(73): training_save_iters = 500
	(74): training_show_iters = 100
	(75): training_start_iters = 0
	(76): training_stats_iters = 1000
	(77): warmup_epochs = 1
	(78): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from saved_models/CCD_finetune_100epochs_ViCalligraphy_base/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 32
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 32
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = saved_models/CCD_finetune_100epochs_ViCalligraphy_base/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_pretrain_checkpoint = ../../../data/CCD/Base_ARD_checkpoint.pth
	(53): model_strict = True
	(54): mp_num = 4
	(55): num_workers = 8
	(56): optimizer = adamw
	(57): optimizer_args_betas = (0.9, 0.999)
	(58): optimizer_bn_wd = False
	(59): optimizer_clip_grad = 20
	(60): optimizer_lr = 0.0001
	(61): optimizer_scheduler_gamma = 0.1
	(62): optimizer_scheduler_periods = [3, 1, 1]
	(63): optimizer_true_wd = False
	(64): optimizer_type = Adam
	(65): optimizer_wd = 0.0
	(66): out_dim = 65536
	(67): output_dir = ./saved_models/
	(68): patch_size = 4
	(69): seed = 0
	(70): training_epochs = 100
	(71): training_eval_iters = 500
	(72): training_hist_iters = 10000000
	(73): training_save_iters = 500
	(74): training_show_iters = 100
	(75): training_start_iters = 0
	(76): training_stats_iters = 1000
	(77): warmup_epochs = 1
	(78): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from saved_models/CCD_finetune_100epochs_ViCalligraphy_base/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = saved_models/CCD_finetune_100epochs_ViCalligraphy_base/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_pretrain_checkpoint = ../../../data/CCD/Base_ARD_checkpoint.pth
	(53): model_strict = True
	(54): mp_num = 4
	(55): num_workers = 8
	(56): optimizer = adamw
	(57): optimizer_args_betas = (0.9, 0.999)
	(58): optimizer_bn_wd = False
	(59): optimizer_clip_grad = 20
	(60): optimizer_lr = 0.0001
	(61): optimizer_scheduler_gamma = 0.1
	(62): optimizer_scheduler_periods = [3, 1, 1]
	(63): optimizer_true_wd = False
	(64): optimizer_type = Adam
	(65): optimizer_wd = 0.0
	(66): out_dim = 65536
	(67): output_dir = ./saved_models/
	(68): patch_size = 4
	(69): seed = 0
	(70): training_epochs = 100
	(71): training_eval_iters = 500
	(72): training_hist_iters = 10000000
	(73): training_save_iters = 500
	(74): training_show_iters = 100
	(75): training_start_iters = 0
	(76): training_stats_iters = 1000
	(77): warmup_epochs = 1
	(78): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/training-->12431'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 296
Read pretrain vision model from ../../../data/CCD/Base_ARD_checkpoint.pth.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 8
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 500
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 500
	(73): training_show_iters = 100
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/training-->12431'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 296
Read vision model from pretrained_model/Base_ARD_checkpoint.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> train loss:5.59096622467041
eval model
iteration:100--> train loss:3.545058488845825
iteration:200--> train loss:1.6909972429275513
iteration:300--> train loss:1.135980248451233
iteration:400--> train loss:0.9456951022148132
iteration:500--> train loss:0.8356403112411499
eval model
iteration:600--> train loss:0.7440614700317383
iteration:700--> train loss:0.6467894315719604
iteration:800--> train loss:0.6266405582427979
iteration:900--> train loss:0.6098885536193848
iteration:1000--> train loss:0.5689921975135803
eval model
iteration:1100--> train loss:0.5640281438827515
iteration:1200--> train loss:0.5298930406570435
iteration:1300--> train loss:0.514141321182251
iteration:1400--> train loss:0.526852011680603
iteration:1500--> train loss:0.5098559260368347
eval model
iteration:1600--> train loss:0.4569009244441986
iteration:1700--> train loss:0.48191002011299133
iteration:1800--> train loss:0.46620237827301025
iteration:1900--> train loss:0.4621592164039612
iteration:2000--> train loss:0.47219061851501465
eval model
iteration:2100--> train loss:0.46590036153793335
iteration:2200--> train loss:0.4129822552204132
iteration:2300--> train loss:0.43940305709838867
iteration:2400--> train loss:0.42998871207237244
iteration:2500--> train loss:0.3952895998954773
eval model
iteration:2600--> train loss:0.4432374835014343
iteration:2700--> train loss:0.408795565366745
iteration:2800--> train loss:0.41309401392936707
iteration:2900--> train loss:0.43489620089530945
iteration:3000--> train loss:0.4106631875038147
eval model
iteration:3100--> train loss:0.39403486251831055
iteration:3200--> train loss:0.40313005447387695
iteration:3300--> train loss:0.40061652660369873
iteration:3400--> train loss:0.3929501175880432
iteration:3500--> train loss:0.408169150352478
eval model
