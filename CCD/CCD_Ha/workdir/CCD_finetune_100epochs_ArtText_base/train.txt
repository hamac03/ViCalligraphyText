ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['../../data/ATTW/artonly/lmdb/test_filtered/']
	(22): dataset_train_batch_size = 48
	(23): dataset_train_roots = ['../../data/ATTW/artonly/lmdb/train_filtered/']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 48
	(28): dataset_valid_roots = ['../../data/ATTW/artonly/lmdb/test_filtered/']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ArtText_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ArtText_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = None
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_pretrain_checkpoint = ../../../data/CCD/Base_ARD_checkpoint.pth
	(53): model_strict = True
	(54): mp_num = 4
	(55): num_workers = 8
	(56): optimizer = adamw
	(57): optimizer_args_betas = (0.9, 0.999)
	(58): optimizer_bn_wd = False
	(59): optimizer_clip_grad = 20
	(60): optimizer_lr = 0.0001
	(61): optimizer_scheduler_gamma = 0.1
	(62): optimizer_scheduler_periods = [3, 1, 1]
	(63): optimizer_true_wd = False
	(64): optimizer_type = Adam
	(65): optimizer_wd = 0.0
	(66): out_dim = 65536
	(67): output_dir = ./saved_models/
	(68): patch_size = 4
	(69): seed = 0
	(70): training_epochs = 100
	(71): training_eval_iters = 5000
	(72): training_hist_iters = 10000000
	(73): training_save_iters = 5000
	(74): training_show_iters = 100
	(75): training_start_iters = 0
	(76): training_stats_iters = 1000
	(77): warmup_epochs = 1
	(78): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:../../data/ATTW/artonly/lmdb/train_filtered/-->12404'

'current_dataset_path:../../data/ATTW/artonly/lmdb/test_filtered/-->4130'

'current_dataset_path:../../data/ATTW/artonly/lmdb/test_filtered/-->4130'

each epoch iteration: 259
Read pretrain vision model from ../../../data/CCD/Base_ARD_checkpoint.pth.
module.backbone.cls_token
module.backbone.pos_embed
module.backbone.patch_embed.proj.weight
module.backbone.patch_embed.proj.bias
module.backbone.blocks.0.norm1.weight
module.backbone.blocks.0.norm1.bias
module.backbone.blocks.0.attn.qkv.weight
module.backbone.blocks.0.attn.qkv.bias
module.backbone.blocks.0.attn.proj.weight
module.backbone.blocks.0.attn.proj.bias
module.backbone.blocks.0.norm2.weight
module.backbone.blocks.0.norm2.bias
module.backbone.blocks.0.mlp.fc1.weight
module.backbone.blocks.0.mlp.fc1.bias
module.backbone.blocks.0.mlp.fc2.weight
module.backbone.blocks.0.mlp.fc2.bias
module.backbone.blocks.1.norm1.weight
module.backbone.blocks.1.norm1.bias
module.backbone.blocks.1.attn.qkv.weight
module.backbone.blocks.1.attn.qkv.bias
module.backbone.blocks.1.attn.proj.weight
module.backbone.blocks.1.attn.proj.bias
module.backbone.blocks.1.norm2.weight
module.backbone.blocks.1.norm2.bias
module.backbone.blocks.1.mlp.fc1.weight
module.backbone.blocks.1.mlp.fc1.bias
module.backbone.blocks.1.mlp.fc2.weight
module.backbone.blocks.1.mlp.fc2.bias
module.backbone.blocks.2.norm1.weight
module.backbone.blocks.2.norm1.bias
module.backbone.blocks.2.attn.qkv.weight
module.backbone.blocks.2.attn.qkv.bias
module.backbone.blocks.2.attn.proj.weight
module.backbone.blocks.2.attn.proj.bias
module.backbone.blocks.2.norm2.weight
module.backbone.blocks.2.norm2.bias
module.backbone.blocks.2.mlp.fc1.weight
module.backbone.blocks.2.mlp.fc1.bias
module.backbone.blocks.2.mlp.fc2.weight
module.backbone.blocks.2.mlp.fc2.bias
module.backbone.blocks.3.norm1.weight
module.backbone.blocks.3.norm1.bias
module.backbone.blocks.3.attn.qkv.weight
module.backbone.blocks.3.attn.qkv.bias
module.backbone.blocks.3.attn.proj.weight
module.backbone.blocks.3.attn.proj.bias
module.backbone.blocks.3.norm2.weight
module.backbone.blocks.3.norm2.bias
module.backbone.blocks.3.mlp.fc1.weight
module.backbone.blocks.3.mlp.fc1.bias
module.backbone.blocks.3.mlp.fc2.weight
module.backbone.blocks.3.mlp.fc2.bias
module.backbone.blocks.4.norm1.weight
module.backbone.blocks.4.norm1.bias
module.backbone.blocks.4.attn.qkv.weight
module.backbone.blocks.4.attn.qkv.bias
module.backbone.blocks.4.attn.proj.weight
module.backbone.blocks.4.attn.proj.bias
module.backbone.blocks.4.norm2.weight
module.backbone.blocks.4.norm2.bias
module.backbone.blocks.4.mlp.fc1.weight
module.backbone.blocks.4.mlp.fc1.bias
module.backbone.blocks.4.mlp.fc2.weight
module.backbone.blocks.4.mlp.fc2.bias
module.backbone.blocks.5.norm1.weight
module.backbone.blocks.5.norm1.bias
module.backbone.blocks.5.attn.qkv.weight
module.backbone.blocks.5.attn.qkv.bias
module.backbone.blocks.5.attn.proj.weight
module.backbone.blocks.5.attn.proj.bias
module.backbone.blocks.5.norm2.weight
module.backbone.blocks.5.norm2.bias
module.backbone.blocks.5.mlp.fc1.weight
module.backbone.blocks.5.mlp.fc1.bias
module.backbone.blocks.5.mlp.fc2.weight
module.backbone.blocks.5.mlp.fc2.bias
module.backbone.blocks.6.norm1.weight
module.backbone.blocks.6.norm1.bias
module.backbone.blocks.6.attn.qkv.weight
module.backbone.blocks.6.attn.qkv.bias
module.backbone.blocks.6.attn.proj.weight
module.backbone.blocks.6.attn.proj.bias
module.backbone.blocks.6.norm2.weight
module.backbone.blocks.6.norm2.bias
module.backbone.blocks.6.mlp.fc1.weight
module.backbone.blocks.6.mlp.fc1.bias
module.backbone.blocks.6.mlp.fc2.weight
module.backbone.blocks.6.mlp.fc2.bias
module.backbone.blocks.7.norm1.weight
module.backbone.blocks.7.norm1.bias
module.backbone.blocks.7.attn.qkv.weight
module.backbone.blocks.7.attn.qkv.bias
module.backbone.blocks.7.attn.proj.weight
module.backbone.blocks.7.attn.proj.bias
module.backbone.blocks.7.norm2.weight
module.backbone.blocks.7.norm2.bias
module.backbone.blocks.7.mlp.fc1.weight
module.backbone.blocks.7.mlp.fc1.bias
module.backbone.blocks.7.mlp.fc2.weight
module.backbone.blocks.7.mlp.fc2.bias
module.backbone.blocks.8.norm1.weight
module.backbone.blocks.8.norm1.bias
module.backbone.blocks.8.attn.qkv.weight
module.backbone.blocks.8.attn.qkv.bias
module.backbone.blocks.8.attn.proj.weight
module.backbone.blocks.8.attn.proj.bias
module.backbone.blocks.8.norm2.weight
module.backbone.blocks.8.norm2.bias
module.backbone.blocks.8.mlp.fc1.weight
module.backbone.blocks.8.mlp.fc1.bias
module.backbone.blocks.8.mlp.fc2.weight
module.backbone.blocks.8.mlp.fc2.bias
module.backbone.blocks.9.norm1.weight
module.backbone.blocks.9.norm1.bias
module.backbone.blocks.9.attn.qkv.weight
module.backbone.blocks.9.attn.qkv.bias
module.backbone.blocks.9.attn.proj.weight
module.backbone.blocks.9.attn.proj.bias
module.backbone.blocks.9.norm2.weight
module.backbone.blocks.9.norm2.bias
module.backbone.blocks.9.mlp.fc1.weight
module.backbone.blocks.9.mlp.fc1.bias
module.backbone.blocks.9.mlp.fc2.weight
module.backbone.blocks.9.mlp.fc2.bias
module.backbone.blocks.10.norm1.weight
module.backbone.blocks.10.norm1.bias
module.backbone.blocks.10.attn.qkv.weight
module.backbone.blocks.10.attn.qkv.bias
module.backbone.blocks.10.attn.proj.weight
module.backbone.blocks.10.attn.proj.bias
module.backbone.blocks.10.norm2.weight
module.backbone.blocks.10.norm2.bias
module.backbone.blocks.10.mlp.fc1.weight
module.backbone.blocks.10.mlp.fc1.bias
module.backbone.blocks.10.mlp.fc2.weight
module.backbone.blocks.10.mlp.fc2.bias
module.backbone.blocks.11.norm1.weight
module.backbone.blocks.11.norm1.bias
module.backbone.blocks.11.attn.qkv.weight
module.backbone.blocks.11.attn.qkv.bias
module.backbone.blocks.11.attn.proj.weight
module.backbone.blocks.11.attn.proj.bias
module.backbone.blocks.11.norm2.weight
module.backbone.blocks.11.norm2.bias
module.backbone.blocks.11.mlp.fc1.weight
module.backbone.blocks.11.mlp.fc1.bias
module.backbone.blocks.11.mlp.fc2.weight
module.backbone.blocks.11.mlp.fc2.bias
module.backbone.norm.weight
module.backbone.norm.bias
module.backbone.norm_seg.0.weight
module.backbone.norm_seg.0.bias
module.backbone.norm_seg.1.weight
module.backbone.norm_seg.1.bias
module.backbone.norm_seg.2.weight
module.backbone.norm_seg.2.bias
module.encoder.fc1.weight
module.encoder.fc1.bias
module.encoder.fc2.weight
module.encoder.fc2.bias
module.decoder.trg_word_emb.weight
module.decoder.position_enc.position_table
module.decoder.layer_stack.0.norm1.weight
module.decoder.layer_stack.0.norm1.bias
module.decoder.layer_stack.0.norm2.weight
module.decoder.layer_stack.0.norm2.bias
module.decoder.layer_stack.0.norm3.weight
module.decoder.layer_stack.0.norm3.bias
module.decoder.layer_stack.0.self_attn.linear_q.weight
module.decoder.layer_stack.0.self_attn.linear_k.weight
module.decoder.layer_stack.0.self_attn.linear_v.weight
module.decoder.layer_stack.0.self_attn.fc.weight
module.decoder.layer_stack.0.enc_attn.linear_q.weight
module.decoder.layer_stack.0.enc_attn.linear_k.weight
module.decoder.layer_stack.0.enc_attn.linear_v.weight
module.decoder.layer_stack.0.enc_attn.fc.weight
module.decoder.layer_stack.0.mlp.w_1.weight
module.decoder.layer_stack.0.mlp.w_1.bias
module.decoder.layer_stack.0.mlp.w_2.weight
module.decoder.layer_stack.0.mlp.w_2.bias
module.decoder.layer_stack.1.norm1.weight
module.decoder.layer_stack.1.norm1.bias
module.decoder.layer_stack.1.norm2.weight
module.decoder.layer_stack.1.norm2.bias
module.decoder.layer_stack.1.norm3.weight
module.decoder.layer_stack.1.norm3.bias
module.decoder.layer_stack.1.self_attn.linear_q.weight
module.decoder.layer_stack.1.self_attn.linear_k.weight
module.decoder.layer_stack.1.self_attn.linear_v.weight
module.decoder.layer_stack.1.self_attn.fc.weight
module.decoder.layer_stack.1.enc_attn.linear_q.weight
module.decoder.layer_stack.1.enc_attn.linear_k.weight
module.decoder.layer_stack.1.enc_attn.linear_v.weight
module.decoder.layer_stack.1.enc_attn.fc.weight
module.decoder.layer_stack.1.mlp.w_1.weight
module.decoder.layer_stack.1.mlp.w_1.bias
module.decoder.layer_stack.1.mlp.w_2.weight
module.decoder.layer_stack.1.mlp.w_2.bias
module.decoder.layer_stack.2.norm1.weight
module.decoder.layer_stack.2.norm1.bias
module.decoder.layer_stack.2.norm2.weight
module.decoder.layer_stack.2.norm2.bias
module.decoder.layer_stack.2.norm3.weight
module.decoder.layer_stack.2.norm3.bias
module.decoder.layer_stack.2.self_attn.linear_q.weight
module.decoder.layer_stack.2.self_attn.linear_k.weight
module.decoder.layer_stack.2.self_attn.linear_v.weight
module.decoder.layer_stack.2.self_attn.fc.weight
module.decoder.layer_stack.2.enc_attn.linear_q.weight
module.decoder.layer_stack.2.enc_attn.linear_k.weight
module.decoder.layer_stack.2.enc_attn.linear_v.weight
module.decoder.layer_stack.2.enc_attn.fc.weight
module.decoder.layer_stack.2.mlp.w_1.weight
module.decoder.layer_stack.2.mlp.w_1.bias
module.decoder.layer_stack.2.mlp.w_2.weight
module.decoder.layer_stack.2.mlp.w_2.bias
module.decoder.layer_stack.3.norm1.weight
module.decoder.layer_stack.3.norm1.bias
module.decoder.layer_stack.3.norm2.weight
module.decoder.layer_stack.3.norm2.bias
module.decoder.layer_stack.3.norm3.weight
module.decoder.layer_stack.3.norm3.bias
module.decoder.layer_stack.3.self_attn.linear_q.weight
module.decoder.layer_stack.3.self_attn.linear_k.weight
module.decoder.layer_stack.3.self_attn.linear_v.weight
module.decoder.layer_stack.3.self_attn.fc.weight
module.decoder.layer_stack.3.enc_attn.linear_q.weight
module.decoder.layer_stack.3.enc_attn.linear_k.weight
module.decoder.layer_stack.3.enc_attn.linear_v.weight
module.decoder.layer_stack.3.enc_attn.fc.weight
module.decoder.layer_stack.3.mlp.w_1.weight
module.decoder.layer_stack.3.mlp.w_1.bias
module.decoder.layer_stack.3.mlp.w_2.weight
module.decoder.layer_stack.3.mlp.w_2.bias
module.decoder.layer_stack.4.norm1.weight
module.decoder.layer_stack.4.norm1.bias
module.decoder.layer_stack.4.norm2.weight
module.decoder.layer_stack.4.norm2.bias
module.decoder.layer_stack.4.norm3.weight
module.decoder.layer_stack.4.norm3.bias
module.decoder.layer_stack.4.self_attn.linear_q.weight
module.decoder.layer_stack.4.self_attn.linear_k.weight
module.decoder.layer_stack.4.self_attn.linear_v.weight
module.decoder.layer_stack.4.self_attn.fc.weight
module.decoder.layer_stack.4.enc_attn.linear_q.weight
module.decoder.layer_stack.4.enc_attn.linear_k.weight
module.decoder.layer_stack.4.enc_attn.linear_v.weight
module.decoder.layer_stack.4.enc_attn.fc.weight
module.decoder.layer_stack.4.mlp.w_1.weight
module.decoder.layer_stack.4.mlp.w_1.bias
module.decoder.layer_stack.4.mlp.w_2.weight
module.decoder.layer_stack.4.mlp.w_2.bias
module.decoder.layer_stack.5.norm1.weight
module.decoder.layer_stack.5.norm1.bias
module.decoder.layer_stack.5.norm2.weight
module.decoder.layer_stack.5.norm2.bias
module.decoder.layer_stack.5.norm3.weight
module.decoder.layer_stack.5.norm3.bias
module.decoder.layer_stack.5.self_attn.linear_q.weight
module.decoder.layer_stack.5.self_attn.linear_k.weight
module.decoder.layer_stack.5.self_attn.linear_v.weight
module.decoder.layer_stack.5.self_attn.fc.weight
module.decoder.layer_stack.5.enc_attn.linear_q.weight
module.decoder.layer_stack.5.enc_attn.linear_k.weight
module.decoder.layer_stack.5.enc_attn.linear_v.weight
module.decoder.layer_stack.5.enc_attn.fc.weight
module.decoder.layer_stack.5.mlp.w_1.weight
module.decoder.layer_stack.5.mlp.w_1.bias
module.decoder.layer_stack.5.mlp.w_2.weight
module.decoder.layer_stack.5.mlp.w_2.bias
module.decoder.layer_norm.weight
module.decoder.layer_norm.bias
module.decoder.classifier.weight
module.decoder.classifier.bias
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> train loss:5.632351875305176
eval model
iteration:100--> train loss:3.9036178588867188
iteration:200--> train loss:3.005835771560669
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['../../data/ATTW/artonly/lmdb/test_filtered/']
	(22): dataset_train_batch_size = 48
	(23): dataset_train_roots = ['../../data/ATTW/artonly/lmdb/train_filtered/']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 48
	(28): dataset_valid_roots = ['../../data/ATTW/artonly/lmdb/test_filtered/']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ArtText_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ArtText_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = None
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_pretrain_checkpoint = ../../../data/CCD/Base_ARD_checkpoint.pth
	(53): model_strict = True
	(54): mp_num = 4
	(55): num_workers = 8
	(56): optimizer = adamw
	(57): optimizer_args_betas = (0.9, 0.999)
	(58): optimizer_bn_wd = False
	(59): optimizer_clip_grad = 20
	(60): optimizer_lr = 0.0001
	(61): optimizer_scheduler_gamma = 0.1
	(62): optimizer_scheduler_periods = [3, 1, 1]
	(63): optimizer_true_wd = False
	(64): optimizer_type = Adam
	(65): optimizer_wd = 0.0
	(66): out_dim = 65536
	(67): output_dir = ./saved_models/
	(68): patch_size = 4
	(69): seed = 0
	(70): training_epochs = 100
	(71): training_eval_iters = 5000
	(72): training_hist_iters = 10000000
	(73): training_save_iters = 5000
	(74): training_show_iters = 100
	(75): training_start_iters = 0
	(76): training_stats_iters = 1000
	(77): warmup_epochs = 1
	(78): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:../../data/ATTW/artonly/lmdb/train_filtered/-->12404'

'current_dataset_path:../../data/ATTW/artonly/lmdb/test_filtered/-->4130'

'current_dataset_path:../../data/ATTW/artonly/lmdb/test_filtered/-->4130'

each epoch iteration: 259
Read pretrain vision model from ../../../data/CCD/Base_ARD_checkpoint.pth.
module.backbone.cls_token
module.backbone.pos_embed
module.backbone.patch_embed.proj.weight
module.backbone.patch_embed.proj.bias
module.backbone.blocks.0.norm1.weight
module.backbone.blocks.0.norm1.bias
module.backbone.blocks.0.attn.qkv.weight
module.backbone.blocks.0.attn.qkv.bias
module.backbone.blocks.0.attn.proj.weight
module.backbone.blocks.0.attn.proj.bias
module.backbone.blocks.0.norm2.weight
module.backbone.blocks.0.norm2.bias
module.backbone.blocks.0.mlp.fc1.weight
module.backbone.blocks.0.mlp.fc1.bias
module.backbone.blocks.0.mlp.fc2.weight
module.backbone.blocks.0.mlp.fc2.bias
module.backbone.blocks.1.norm1.weight
module.backbone.blocks.1.norm1.bias
module.backbone.blocks.1.attn.qkv.weight
module.backbone.blocks.1.attn.qkv.bias
module.backbone.blocks.1.attn.proj.weight
module.backbone.blocks.1.attn.proj.bias
module.backbone.blocks.1.norm2.weight
module.backbone.blocks.1.norm2.bias
module.backbone.blocks.1.mlp.fc1.weight
module.backbone.blocks.1.mlp.fc1.bias
module.backbone.blocks.1.mlp.fc2.weight
module.backbone.blocks.1.mlp.fc2.bias
module.backbone.blocks.2.norm1.weight
module.backbone.blocks.2.norm1.bias
module.backbone.blocks.2.attn.qkv.weight
module.backbone.blocks.2.attn.qkv.bias
module.backbone.blocks.2.attn.proj.weight
module.backbone.blocks.2.attn.proj.bias
module.backbone.blocks.2.norm2.weight
module.backbone.blocks.2.norm2.bias
module.backbone.blocks.2.mlp.fc1.weight
module.backbone.blocks.2.mlp.fc1.bias
module.backbone.blocks.2.mlp.fc2.weight
module.backbone.blocks.2.mlp.fc2.bias
module.backbone.blocks.3.norm1.weight
module.backbone.blocks.3.norm1.bias
module.backbone.blocks.3.attn.qkv.weight
module.backbone.blocks.3.attn.qkv.bias
module.backbone.blocks.3.attn.proj.weight
module.backbone.blocks.3.attn.proj.bias
module.backbone.blocks.3.norm2.weight
module.backbone.blocks.3.norm2.bias
module.backbone.blocks.3.mlp.fc1.weight
module.backbone.blocks.3.mlp.fc1.bias
module.backbone.blocks.3.mlp.fc2.weight
module.backbone.blocks.3.mlp.fc2.bias
module.backbone.blocks.4.norm1.weight
module.backbone.blocks.4.norm1.bias
module.backbone.blocks.4.attn.qkv.weight
module.backbone.blocks.4.attn.qkv.bias
module.backbone.blocks.4.attn.proj.weight
module.backbone.blocks.4.attn.proj.bias
module.backbone.blocks.4.norm2.weight
module.backbone.blocks.4.norm2.bias
module.backbone.blocks.4.mlp.fc1.weight
module.backbone.blocks.4.mlp.fc1.bias
module.backbone.blocks.4.mlp.fc2.weight
module.backbone.blocks.4.mlp.fc2.bias
module.backbone.blocks.5.norm1.weight
module.backbone.blocks.5.norm1.bias
module.backbone.blocks.5.attn.qkv.weight
module.backbone.blocks.5.attn.qkv.bias
module.backbone.blocks.5.attn.proj.weight
module.backbone.blocks.5.attn.proj.bias
module.backbone.blocks.5.norm2.weight
module.backbone.blocks.5.norm2.bias
module.backbone.blocks.5.mlp.fc1.weight
module.backbone.blocks.5.mlp.fc1.bias
module.backbone.blocks.5.mlp.fc2.weight
module.backbone.blocks.5.mlp.fc2.bias
module.backbone.blocks.6.norm1.weight
module.backbone.blocks.6.norm1.bias
module.backbone.blocks.6.attn.qkv.weight
module.backbone.blocks.6.attn.qkv.bias
module.backbone.blocks.6.attn.proj.weight
module.backbone.blocks.6.attn.proj.bias
module.backbone.blocks.6.norm2.weight
module.backbone.blocks.6.norm2.bias
module.backbone.blocks.6.mlp.fc1.weight
module.backbone.blocks.6.mlp.fc1.bias
module.backbone.blocks.6.mlp.fc2.weight
module.backbone.blocks.6.mlp.fc2.bias
module.backbone.blocks.7.norm1.weight
module.backbone.blocks.7.norm1.bias
module.backbone.blocks.7.attn.qkv.weight
module.backbone.blocks.7.attn.qkv.bias
module.backbone.blocks.7.attn.proj.weight
module.backbone.blocks.7.attn.proj.bias
module.backbone.blocks.7.norm2.weight
module.backbone.blocks.7.norm2.bias
module.backbone.blocks.7.mlp.fc1.weight
module.backbone.blocks.7.mlp.fc1.bias
module.backbone.blocks.7.mlp.fc2.weight
module.backbone.blocks.7.mlp.fc2.bias
module.backbone.blocks.8.norm1.weight
module.backbone.blocks.8.norm1.bias
module.backbone.blocks.8.attn.qkv.weight
module.backbone.blocks.8.attn.qkv.bias
module.backbone.blocks.8.attn.proj.weight
module.backbone.blocks.8.attn.proj.bias
module.backbone.blocks.8.norm2.weight
module.backbone.blocks.8.norm2.bias
module.backbone.blocks.8.mlp.fc1.weight
module.backbone.blocks.8.mlp.fc1.bias
module.backbone.blocks.8.mlp.fc2.weight
module.backbone.blocks.8.mlp.fc2.bias
module.backbone.blocks.9.norm1.weight
module.backbone.blocks.9.norm1.bias
module.backbone.blocks.9.attn.qkv.weight
module.backbone.blocks.9.attn.qkv.bias
module.backbone.blocks.9.attn.proj.weight
module.backbone.blocks.9.attn.proj.bias
module.backbone.blocks.9.norm2.weight
module.backbone.blocks.9.norm2.bias
module.backbone.blocks.9.mlp.fc1.weight
module.backbone.blocks.9.mlp.fc1.bias
module.backbone.blocks.9.mlp.fc2.weight
module.backbone.blocks.9.mlp.fc2.bias
module.backbone.blocks.10.norm1.weight
module.backbone.blocks.10.norm1.bias
module.backbone.blocks.10.attn.qkv.weight
module.backbone.blocks.10.attn.qkv.bias
module.backbone.blocks.10.attn.proj.weight
module.backbone.blocks.10.attn.proj.bias
module.backbone.blocks.10.norm2.weight
module.backbone.blocks.10.norm2.bias
module.backbone.blocks.10.mlp.fc1.weight
module.backbone.blocks.10.mlp.fc1.bias
module.backbone.blocks.10.mlp.fc2.weight
module.backbone.blocks.10.mlp.fc2.bias
module.backbone.blocks.11.norm1.weight
module.backbone.blocks.11.norm1.bias
module.backbone.blocks.11.attn.qkv.weight
module.backbone.blocks.11.attn.qkv.bias
module.backbone.blocks.11.attn.proj.weight
module.backbone.blocks.11.attn.proj.bias
module.backbone.blocks.11.norm2.weight
module.backbone.blocks.11.norm2.bias
module.backbone.blocks.11.mlp.fc1.weight
module.backbone.blocks.11.mlp.fc1.bias
module.backbone.blocks.11.mlp.fc2.weight
module.backbone.blocks.11.mlp.fc2.bias
module.backbone.norm.weight
module.backbone.norm.bias
module.backbone.norm_seg.0.weight
module.backbone.norm_seg.0.bias
module.backbone.norm_seg.1.weight
module.backbone.norm_seg.1.bias
module.backbone.norm_seg.2.weight
module.backbone.norm_seg.2.bias
module.encoder.fc1.weight
module.encoder.fc1.bias
module.encoder.fc2.weight
module.encoder.fc2.bias
module.decoder.trg_word_emb.weight
module.decoder.position_enc.position_table
module.decoder.layer_stack.0.norm1.weight
module.decoder.layer_stack.0.norm1.bias
module.decoder.layer_stack.0.norm2.weight
module.decoder.layer_stack.0.norm2.bias
module.decoder.layer_stack.0.norm3.weight
module.decoder.layer_stack.0.norm3.bias
module.decoder.layer_stack.0.self_attn.linear_q.weight
module.decoder.layer_stack.0.self_attn.linear_k.weight
module.decoder.layer_stack.0.self_attn.linear_v.weight
module.decoder.layer_stack.0.self_attn.fc.weight
module.decoder.layer_stack.0.enc_attn.linear_q.weight
module.decoder.layer_stack.0.enc_attn.linear_k.weight
module.decoder.layer_stack.0.enc_attn.linear_v.weight
module.decoder.layer_stack.0.enc_attn.fc.weight
module.decoder.layer_stack.0.mlp.w_1.weight
module.decoder.layer_stack.0.mlp.w_1.bias
module.decoder.layer_stack.0.mlp.w_2.weight
module.decoder.layer_stack.0.mlp.w_2.bias
module.decoder.layer_stack.1.norm1.weight
module.decoder.layer_stack.1.norm1.bias
module.decoder.layer_stack.1.norm2.weight
module.decoder.layer_stack.1.norm2.bias
module.decoder.layer_stack.1.norm3.weight
module.decoder.layer_stack.1.norm3.bias
module.decoder.layer_stack.1.self_attn.linear_q.weight
module.decoder.layer_stack.1.self_attn.linear_k.weight
module.decoder.layer_stack.1.self_attn.linear_v.weight
module.decoder.layer_stack.1.self_attn.fc.weight
module.decoder.layer_stack.1.enc_attn.linear_q.weight
module.decoder.layer_stack.1.enc_attn.linear_k.weight
module.decoder.layer_stack.1.enc_attn.linear_v.weight
module.decoder.layer_stack.1.enc_attn.fc.weight
module.decoder.layer_stack.1.mlp.w_1.weight
module.decoder.layer_stack.1.mlp.w_1.bias
module.decoder.layer_stack.1.mlp.w_2.weight
module.decoder.layer_stack.1.mlp.w_2.bias
module.decoder.layer_stack.2.norm1.weight
module.decoder.layer_stack.2.norm1.bias
module.decoder.layer_stack.2.norm2.weight
module.decoder.layer_stack.2.norm2.bias
module.decoder.layer_stack.2.norm3.weight
module.decoder.layer_stack.2.norm3.bias
module.decoder.layer_stack.2.self_attn.linear_q.weight
module.decoder.layer_stack.2.self_attn.linear_k.weight
module.decoder.layer_stack.2.self_attn.linear_v.weight
module.decoder.layer_stack.2.self_attn.fc.weight
module.decoder.layer_stack.2.enc_attn.linear_q.weight
module.decoder.layer_stack.2.enc_attn.linear_k.weight
module.decoder.layer_stack.2.enc_attn.linear_v.weight
module.decoder.layer_stack.2.enc_attn.fc.weight
module.decoder.layer_stack.2.mlp.w_1.weight
module.decoder.layer_stack.2.mlp.w_1.bias
module.decoder.layer_stack.2.mlp.w_2.weight
module.decoder.layer_stack.2.mlp.w_2.bias
module.decoder.layer_stack.3.norm1.weight
module.decoder.layer_stack.3.norm1.bias
module.decoder.layer_stack.3.norm2.weight
module.decoder.layer_stack.3.norm2.bias
module.decoder.layer_stack.3.norm3.weight
module.decoder.layer_stack.3.norm3.bias
module.decoder.layer_stack.3.self_attn.linear_q.weight
module.decoder.layer_stack.3.self_attn.linear_k.weight
module.decoder.layer_stack.3.self_attn.linear_v.weight
module.decoder.layer_stack.3.self_attn.fc.weight
module.decoder.layer_stack.3.enc_attn.linear_q.weight
module.decoder.layer_stack.3.enc_attn.linear_k.weight
module.decoder.layer_stack.3.enc_attn.linear_v.weight
module.decoder.layer_stack.3.enc_attn.fc.weight
module.decoder.layer_stack.3.mlp.w_1.weight
module.decoder.layer_stack.3.mlp.w_1.bias
module.decoder.layer_stack.3.mlp.w_2.weight
module.decoder.layer_stack.3.mlp.w_2.bias
module.decoder.layer_stack.4.norm1.weight
module.decoder.layer_stack.4.norm1.bias
module.decoder.layer_stack.4.norm2.weight
module.decoder.layer_stack.4.norm2.bias
module.decoder.layer_stack.4.norm3.weight
module.decoder.layer_stack.4.norm3.bias
module.decoder.layer_stack.4.self_attn.linear_q.weight
module.decoder.layer_stack.4.self_attn.linear_k.weight
module.decoder.layer_stack.4.self_attn.linear_v.weight
module.decoder.layer_stack.4.self_attn.fc.weight
module.decoder.layer_stack.4.enc_attn.linear_q.weight
module.decoder.layer_stack.4.enc_attn.linear_k.weight
module.decoder.layer_stack.4.enc_attn.linear_v.weight
module.decoder.layer_stack.4.enc_attn.fc.weight
module.decoder.layer_stack.4.mlp.w_1.weight
module.decoder.layer_stack.4.mlp.w_1.bias
module.decoder.layer_stack.4.mlp.w_2.weight
module.decoder.layer_stack.4.mlp.w_2.bias
module.decoder.layer_stack.5.norm1.weight
module.decoder.layer_stack.5.norm1.bias
module.decoder.layer_stack.5.norm2.weight
module.decoder.layer_stack.5.norm2.bias
module.decoder.layer_stack.5.norm3.weight
module.decoder.layer_stack.5.norm3.bias
module.decoder.layer_stack.5.self_attn.linear_q.weight
module.decoder.layer_stack.5.self_attn.linear_k.weight
module.decoder.layer_stack.5.self_attn.linear_v.weight
module.decoder.layer_stack.5.self_attn.fc.weight
module.decoder.layer_stack.5.enc_attn.linear_q.weight
module.decoder.layer_stack.5.enc_attn.linear_k.weight
module.decoder.layer_stack.5.enc_attn.linear_v.weight
module.decoder.layer_stack.5.enc_attn.fc.weight
module.decoder.layer_stack.5.mlp.w_1.weight
module.decoder.layer_stack.5.mlp.w_1.bias
module.decoder.layer_stack.5.mlp.w_2.weight
module.decoder.layer_stack.5.mlp.w_2.bias
module.decoder.layer_norm.weight
module.decoder.layer_norm.bias
module.decoder.classifier.weight
module.decoder.classifier.bias
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> train loss:5.563112258911133
eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['../../data/ATTW/artonly/lmdb/test_filtered/']
	(22): dataset_train_batch_size = 48
	(23): dataset_train_roots = ['../../data/ATTW/artonly/lmdb/train_filtered/']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 48
	(28): dataset_valid_roots = ['../../data/ATTW/artonly/lmdb/test_filtered/']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ArtText_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ArtText_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = ../../../data/CCD/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 8
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 5000
	(73): training_show_iters = 100
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:../../data/ATTW/artonly/lmdb/train_filtered/-->12404'

'current_dataset_path:../../data/ATTW/artonly/lmdb/test_filtered/-->4130'

'current_dataset_path:../../data/ATTW/artonly/lmdb/test_filtered/-->4130'

each epoch iteration: 259
Read vision model from ../../../data/CCD/Base_ARD_checkpoint.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> train loss:5.459192752838135
eval model
iteration:100--> train loss:3.483160972595215
iteration:200--> train loss:1.1701024770736694
iteration:300--> train loss:1.0129780769348145
iteration:400--> train loss:0.925775945186615
iteration:500--> train loss:0.8300217390060425
iteration:600--> train loss:0.727330207824707
iteration:700--> train loss:0.7656038999557495
iteration:800--> train loss:0.7572006583213806
iteration:900--> train loss:0.6656104922294617
iteration:1000--> train loss:0.6858982443809509
iteration:1100--> train loss:0.6248975992202759
iteration:1200--> train loss:0.6259600520133972
iteration:1300--> train loss:0.6506385803222656
iteration:1400--> train loss:0.5577419996261597
iteration:1500--> train loss:0.6311258673667908
iteration:1600--> train loss:0.5911505222320557
iteration:1700--> train loss:0.5370668172836304
iteration:1800--> train loss:0.5777679681777954
iteration:1900--> train loss:0.5222472548484802
iteration:2000--> train loss:0.5505460500717163
iteration:2100--> train loss:0.5314887762069702
iteration:2200--> train loss:0.5000428557395935
iteration:2300--> train loss:0.5297749042510986
iteration:2400--> train loss:0.49795085191726685
iteration:2500--> train loss:0.4803195893764496
iteration:2600--> train loss:0.5079416036605835
iteration:2700--> train loss:0.4433155357837677
iteration:2800--> train loss:0.47486841678619385
iteration:2900--> train loss:0.4724012315273285
iteration:3000--> train loss:0.44773069024086
iteration:3100--> train loss:0.5036385655403137
iteration:3200--> train loss:0.4371662437915802
iteration:3300--> train loss:0.464762419462204
iteration:3400--> train loss:0.4560448229312897
iteration:3500--> train loss:0.4370711147785187
iteration:3600--> train loss:0.4426468312740326
iteration:3700--> train loss:0.4120953679084778
iteration:3800--> train loss:0.40477365255355835
iteration:3900--> train loss:0.43689918518066406
iteration:4000--> train loss:0.39246320724487305
iteration:4100--> train loss:0.43234190344810486
iteration:4200--> train loss:0.4072677493095398
iteration:4300--> train loss:0.3900805115699768
iteration:4400--> train loss:0.40138816833496094
iteration:4500--> train loss:0.3665062189102173
iteration:4600--> train loss:0.39894142746925354
iteration:4700--> train loss:0.37283122539520264
iteration:4800--> train loss:0.36781585216522217
iteration:4900--> train loss:0.40315553545951843
iteration:5000--> train loss:0.3664019703865051
eval model
iteration:5100--> train loss:0.37981799244880676
iteration:5200--> train loss:0.37352991104125977
iteration:5300--> train loss:0.3489069938659668
iteration:5400--> train loss:0.3633318543434143
iteration:5500--> train loss:0.35963889956474304
iteration:5600--> train loss:0.3285057246685028
iteration:5700--> train loss:0.3672208786010742
iteration:5800--> train loss:0.3191002607345581
iteration:5900--> train loss:0.34758827090263367
iteration:6000--> train loss:0.3518320620059967
iteration:6100--> train loss:0.33498233556747437
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 48
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 48
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ArtText_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ArtText_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 8
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 5000
	(73): training_show_iters = 100
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/training-->12431'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 259
Read vision model from pretrained_model/Base_ARD_checkpoint.pth.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = False
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 48
	(21): dataset_test_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 48
	(23): dataset_train_roots = ['Dino/training_eval_ViCalligraphy/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 48
	(28): dataset_valid_roots = ['Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ArtText_base
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ArtText_base
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 8
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 5000
	(73): training_show_iters = 100
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:Dino/training_eval_ViCalligraphy/training-->12431'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 259
Read vision model from pretrained_model/Base_ARD_checkpoint.pth.
